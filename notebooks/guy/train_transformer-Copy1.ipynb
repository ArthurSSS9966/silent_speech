{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734187a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, glob\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchmetrics\n",
    "\n",
    "import jiwer\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a094152c",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1847199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46c9f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "class SpeechDataset(torch.utils.data.Dataset):   \n",
    "    def __init__(self, X, y, z=None, toggle_phones = False):\n",
    "        '''Inputs are:\n",
    "            X (list of 2D arrays) - entries are time x channels of neural activity\n",
    "            y (list of 1D arrays) - entries are integer-encoded target strings (character or phoneme)\n",
    "            z (list of ints)      - entries are session IDs'''\n",
    "        \n",
    "        assert len(X) == len(y), 'Target and predictor lists must be same length'\n",
    "        if z is not None:\n",
    "            assert len(X) == len(z), 'Target and session ID lists must be same length'\n",
    "            \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "        self.num_features   = self.X[0].shape[1]\n",
    "        self.toggle_phones  = toggle_phones\n",
    "        self.text_transform = TextTransform(toggle_phones = self.toggle_phones)\n",
    "        self.smoothed       = False\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    #@lru_cache\n",
    "    def __getitem__(self, index):        \n",
    "        if self.z is not None:\n",
    "            return (self.X[index], self.y[index], self.z[index])\n",
    "        else:\n",
    "            return (self.X[index], self.y[index])\n",
    "        \n",
    "    def smooth_data(self, sigma):\n",
    "        if not self.smoothed and sigma > 0:\n",
    "            for idx in range(len(self.X)):\n",
    "                self.X[idx] = gaussian_filter1d(self.X[idx], sigma = sigma, causal=True, axis = 0)\n",
    "            self.smoothed = True\n",
    "        else:\n",
    "            print('Warning: data already smoothed. Skipping...')\n",
    "    \n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        '''\n",
    "        Padds batch of variable length\n",
    "        '''\n",
    "        # get sequence lengths\n",
    "        lengths = torch.tensor([t[0].shape[0] for t in batch])\n",
    "        # pad\n",
    "        batch_x = [torch.tensor(t[0]) for t in batch]\n",
    "        batch_x = torch.nn.utils.rnn.pad_sequence(batch_x, batch_first=True)\n",
    "        \n",
    "        datas = dict()\n",
    "        datas['neural']           = batch_x\n",
    "        datas['text_int']         = [torch.tensor(t[1]) for t in batch]\n",
    "        datas['text_int_lengths'] = torch.tensor([ex[ex != 0].shape[0] for ex in datas['text_int']])\n",
    "        datas['lengths']          = lengths\n",
    "        if self.z is not None:\n",
    "            datas['session_ids'] = torch.tensor([t[2] for t in batch])\n",
    "        else:\n",
    "            datas['session_ids'] = None\n",
    "        \n",
    "        return datas\n",
    "\n",
    "\n",
    "def convertNumbersToStrings(sentence):\n",
    "    \n",
    "    output_sentence = []\n",
    "    for word in sentence.split():\n",
    "        if word.isdigit():\n",
    "            output_sentence.append(numToWords(word))\n",
    "        else:\n",
    "            output_sentence.append(word)\n",
    "    output_sentence = ' '.join(output_sentence)\n",
    "\n",
    "    return output_sentence\n",
    "\n",
    "\n",
    "class TextTransform(object):\n",
    "    def __init__(self, toggle_phones = False):\n",
    "        self.togglePhones     = toggle_phones\n",
    "        \n",
    "        self.transformation   = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])\n",
    "        self.replacement_dict = {}\n",
    "        \n",
    "        if self.togglePhones:\n",
    "            self.chars = [\n",
    "                    'AA', 'AE', 'AH', 'AO', 'AW',\n",
    "                    'AY', 'B',  'CH', 'D', 'DH',\n",
    "                    'EH', 'ER', 'EY', 'F', 'G',\n",
    "                    'HH', 'IH', 'IY', 'JH', 'K',\n",
    "                    'L', 'M', 'N', 'NG', 'OW',\n",
    "                    'OY', 'P', 'R', 'S', 'SH',\n",
    "                    'T', 'TH', 'UH', 'UW', 'V',\n",
    "                    'W', 'Y', 'Z', 'ZH', '|'\n",
    "                ]\n",
    "\n",
    "        else:\n",
    "            self.chars = [x for x in string.ascii_lowercase + '|']         \n",
    "\n",
    "    def clean_text(self, text):\n",
    "        if self.togglePhones:\n",
    "            raise NotImplementedError(\"Use preprocessed integer encoding for phone data\")\n",
    "        else:\n",
    "            #text = unidecode(text)\n",
    "            #text = text.replace('-', ' ')\n",
    "            #text = text.replace(':', ' ')\n",
    "            text = self.transformation(text)\n",
    "            text = convertNumbersToStrings(text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        text = self.clean_text(text)\n",
    "        if self.togglePhones:\n",
    "            text = [x.replace(' ', '|') for x in text]\n",
    "        else:\n",
    "            text = text.replace(' ', '|')\n",
    "        return [self.chars.index(c) for c in text]\n",
    "\n",
    "    def int_to_text(self, ints):\n",
    "        text = ''.join(self.chars[i] for i in ints)\n",
    "        text = text.replace('|', ' ').lower()\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "132d41d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersecting files:  []\n",
      "Unique days: 24\n"
     ]
    }
   ],
   "source": [
    "datadir        = '/oak/stanford/groups/shenoy/ghwilson/T12speech/sessions/'\n",
    "\n",
    "train_files = glob.glob(datadir + '*/train/*')\n",
    "test_files  = glob.glob(datadir + '*/test/*')\n",
    "\n",
    "days   = [str(x.split('sessions/')[1].split('/')[0]) for x in train_files + test_files]\n",
    "unique = np.unique(days)\n",
    "session_mapping = dict(zip(unique, np.arange(len(unique))))\n",
    "\n",
    "print('Intersecting files: ', np.intersect1d(train_files, test_files))\n",
    "print('Unique days:', len(session_mapping.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "824d23c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9740\n",
      "2120\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "toggle_phones = True\n",
    "\n",
    "# loading data into CPU, should work but takes ~ 6-10 minutes:\n",
    "datasets  = list()\n",
    "transform = TextTransform(toggle_phones=toggle_phones)\n",
    "\n",
    "for files in [train_files, test_files]:\n",
    "    mat = [scipy.io.loadmat(x) for x in files]\n",
    "    X   = [trl['neural'] for trl in mat]\n",
    "    \n",
    "    if toggle_phones:\n",
    "        y = [trl['phoneme_ints'][trl['phoneme_ints'] != 0] for trl in mat]\n",
    "    else:\n",
    "        y = [transform.text_to_int(trl['text'][0]) for trl in mat]\n",
    "        \n",
    "    z = [session_mapping[f.split('sessions/')[1].split('/')[0]] for f in files]\n",
    "    datasets.append(SpeechDataset(X, y, z, toggle_phones=toggle_phones))\n",
    "    del mat, X, y\n",
    "    \n",
    "\n",
    "trainset = datasets[0]\n",
    "testset  = datasets[1]\n",
    "\n",
    "\n",
    "print(len(trainset))\n",
    "print(len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2a8229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.functional import edit_distance\n",
    "\n",
    "\n",
    "def addRandomWalk(timeseries, strength):\n",
    "    '''Apply mean drift to timeseries data using autoregressive noise. Inputs are:\n",
    "    \n",
    "        timeseries (batch x time x features) - data input\n",
    "        strength (float)                     - noise standard deviation '''\n",
    "    \n",
    "    nBatch, nTime, nChannels = timeseries.shape\n",
    "    noise                    = torch.zeros(timeseries.shape)\n",
    "    noise[:, 0, :]           = torch.normal(torch.zeros((nChannels)), strength) \n",
    "    \n",
    "    for t in range(1, nTime):\n",
    "        noise[:, t, :] = noise[:, t-1, :] + torch.normal(torch.zeros((nBatch, nChannels)), strength)  \n",
    "        \n",
    "    return timeseries + noise\n",
    "\n",
    "def addWhiteNoise(timeseries, strength):\n",
    "    '''Apply IID gaussian noise to timeseries data. Inputs are:\n",
    "\n",
    "    timeseries (batch x time x features) - data input\n",
    "    strength (float)                     - noise standard deviation '''\n",
    "\n",
    "    noise = torch.normal(torch.zeros((timeseries.shape)), strength)    \n",
    "    return timeseries + noise\n",
    "\n",
    "def addOffset(timeseries, strength):\n",
    "    '''Add constant offsets to timeseries data. Inputs are: \n",
    "    \n",
    "        timeseries (batch x time x features) - data input\n",
    "        strength (float)                     - offset standard deviation '''\n",
    "\n",
    "    nBatch, nTime, nChannels = timeseries.shape\n",
    "    offset = torch.normal(torch.zeros((nBatch, nChannels)), strength)  \n",
    "    return timeseries + offset[:, None, :]\n",
    "\n",
    "def addNoise(timeseries, offset_strength = 0, whitenoise_strength = 0, randomwalk_strength = 0):  \n",
    "    '''Interface function for adding various noise types to data.'''\n",
    "    \n",
    "    if offset_strength > 0:\n",
    "        timeseries = addOffset(timeseries, strength = offset_strength)\n",
    "    if whitenoise_strength > 0:\n",
    "        timeseries = addWhiteNoise(timeseries, strength = whitenoise_strength)\n",
    "    if randomwalk_strength > 0:\n",
    "        timeseries = addRandomWalk(timeseries, strength = randomwalk_strength) \n",
    "        \n",
    "    return timeseries\n",
    "\n",
    "\n",
    "def stripLeadingTrailing(arr, val):\n",
    "    '''Remove leading and trailing values from array. E.g.:\n",
    "        \n",
    "        > x = [0, 0, 1, 2, -1, 5, 's', 0]\n",
    "        > stripLeadingTrailing(x) \n",
    "        > [1, 2, -1, 5, 's']\n",
    "    '''\n",
    "    \n",
    "    # if arr is just a sequence of <val>, return a singleton\n",
    "    unique_vals = torch.unique(arr)\n",
    "    if len(unique_vals) == 1 and unique_vals[0] == val:\n",
    "        return torch.tensor([val])\n",
    "    \n",
    "    else:\n",
    "        idx = 0\n",
    "        while arr[idx] == val:\n",
    "            idx += 1\n",
    "\n",
    "        arr = arr[idx:]\n",
    "\n",
    "        # trailing values\n",
    "        idx = -1\n",
    "        while arr[idx] == val:\n",
    "            idx -= 1\n",
    "\n",
    "        arr = arr[:idx]\n",
    "    \n",
    "        return arr\n",
    "    \n",
    "    \n",
    "def buildCTCDecoder(testset, use_lm):\n",
    "    \n",
    "    if testset.toggle_phones:\n",
    "        #lexicon_file = 'cmudict.txt'\n",
    "        #lexicon_file = '/oak/stanford/groups/shenoy/fwillett/speech/cmudict-0.7b.txt'\n",
    "        lexicon_file = None\n",
    "    else:\n",
    "        lexicon_file = os.path.join(FLAGS.lm_directory, 'lexicon_graphemes_noApostrophe.txt')\n",
    "        \n",
    "    if use_lm:\n",
    "        lm = os.path.join(FLAGS.lm_directory, '4gram_lm.bin')\n",
    "    else:\n",
    "        lm = None\n",
    "\n",
    "    decoder = ctc_decoder(\n",
    "       lexicon     = lexicon_file,\n",
    "       tokens      = ['_'] + [x.lower() for x in testset.text_transform.chars],\n",
    "       lm          = lm, \n",
    "       blank_token = '_',\n",
    "       sil_token   = '|',\n",
    "       nbest       = 1,\n",
    "       lm_weight   = 2, \n",
    "       #word_score  = -3,\n",
    "       #sil_score   = -2,\n",
    "       beam_size   = 50  # SET TO 150 during inference\n",
    "    )\n",
    "    \n",
    "    return decoder\n",
    "\n",
    "\n",
    "\n",
    "def test(model, testset, device, use_lm = False):\n",
    "    model.eval()\n",
    "    \n",
    "    decoder = buildCTCDecoder(testset, use_lm)\n",
    "    \n",
    "    dataloader  = torch.utils.data.DataLoader(testset, batch_size=1, collate_fn=testset.collate_fn)\n",
    "    charTargets = []\n",
    "    charPreds   = []\n",
    "    wordTargets = []\n",
    "    wordPreds   = []\n",
    "    with torch.no_grad():\n",
    "        for i, example in enumerate(dataloader):\n",
    "            X_raw   = example['neural'][0].unsqueeze(0).to(device)\n",
    "            session = example['session_ids'][0].unsqueeze(0).to(device)\n",
    "            pred    = F.log_softmax(model(X_raw, session), -1).cpu()\n",
    "            \n",
    "            beam_results = decoder(pred)\n",
    "            pred_int     = beam_results[0][0].tokens\n",
    "            target_int   = example['text_int'][0][:example['text_int_lengths'][0]]\n",
    "            \n",
    "            pred_int   = stripLeadingTrailing(pred_int, 40)\n",
    "            target_int = stripLeadingTrailing(target_int, 40)\n",
    "\n",
    "            \n",
    "            charPreds.append(pred_int)\n",
    "            charTargets.append(target_int)\n",
    "            \n",
    "            if not testset.toggle_phones:\n",
    "                pred_text    = ' '.join(beam_results[0][0].words).strip().lower()\n",
    "                target_text  = testset.text_transform.int_to_text(example['text_int'][0])\n",
    "                \n",
    "                wordPreds.append(pred_text)\n",
    "                wordTargets.append(target_text)\n",
    "\n",
    "            #pred_text    = testset.text_transform.int_to_text(pred_int)\n",
    "            #target_text  = testset.text_transform.clean_2(example['text'][0][0])\n",
    "            \n",
    "            if i < 0:\n",
    "                print('Prediction: ', pred_int)\n",
    "                print('Target: ', target_int)\n",
    "            if i > 100:\n",
    "                # for now only measure 100 sentence for validation\n",
    "                break\n",
    "                \n",
    "    if testset.toggle_phones:\n",
    "        # phone/character error rate computed across all examples\n",
    "        cer  = 0\n",
    "        lens = 0\n",
    "        for x, y in zip(charTargets, charPreds):\n",
    "            cer  += edit_distance(x, y)\n",
    "            lens += len(x)\n",
    "            \n",
    "        cer /= lens\n",
    "        wer = 0\n",
    "        \n",
    "    else:\n",
    "        cer =  0 #jiwer.cer(charTargets, charPreds)  \n",
    "        wer = jiwer.wer(wordTargets, wordPreds)\n",
    "        \n",
    "    return cer, wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759445f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import flags\n",
    "import neptune.new as neptune\n",
    "\n",
    "for name in list(flags.FLAGS):\n",
    "    delattr(flags.FLAGS,name)\n",
    "\n",
    "\n",
    "flags.DEFINE_boolean('debug', False, 'debug')\n",
    "flags.DEFINE_string('output_directory', '/oak/stanford/groups/shenoy/ghwilson/T12_transformer', 'where to save models and outputs')\n",
    "\n",
    "flags.DEFINE_integer('model_size', 768, 'Toggle S4 model in place of transformer') #768\n",
    "flags.DEFINE_integer('num_layers', 5, 'Toggle S4 model in place of transformer') # 6\n",
    "flags.DEFINE_integer('per_session_layer', 0, 'Toggle session-specific input layer')\n",
    "flags.DEFINE_integer('n_conv_blocks', 0, 'Toggle convolutional front-end') #768\n",
    "\n",
    "\n",
    "flags.DEFINE_float('dropout', 0.2, 'dropout strength') # 0.2\n",
    "flags.DEFINE_float('whitenoise_SD', 0.8, 'whitenoise strength')\n",
    "flags.DEFINE_float('constantoffset_SD', 0.2, 'whitenoise strength')\n",
    "flags.DEFINE_float('smoothing_sigma', 2, 'whitenoise strength')\n",
    "\n",
    "\n",
    "flags.DEFINE_integer('batch_size', 32, 'training batch size')\n",
    "flags.DEFINE_float('learning_rate', 2e-4, 'learning rate') #3e-4\n",
    "flags.DEFINE_integer('epochs', 150, 'training epochs')  #200\n",
    "flags.DEFINE_integer('learning_rate_patience', 5, 'learning rate decay patience')\n",
    "flags.DEFINE_string('start_training_from', None, 'start training from this model')\n",
    "flags.DEFINE_float('l2', 0, 'weight decay')\n",
    "\n",
    "flags.DEFINE_string('evaluate_saved', None, 'run evaluation on given model file')\n",
    "flags.DEFINE_string('lm_directory', '/oak/stanford/projects/babelfish/magneto/GaddyPaper/pretrained_models/librispeech_lm/', \n",
    "                    'Path to KenLM language model')\n",
    "flags.DEFINE_string('base_dir', '/oak/stanford/projects/babelfish/magneto/GaddyPaper/processed_data/',\n",
    "                    'path to processed EMG dataset')\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS([''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced7e9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import gaussian_filter1d\n",
    "\n",
    "\n",
    "trainset.smooth_data(FLAGS.smoothing_sigma)\n",
    "testset.smooth_data(FLAGS.smoothing_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37369e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sched = torch.optim.lr_scheduler.OneCycleLR(optim, max_lr = FLAGS.learning_rate, epochs = FLAGS.epochs,\n",
    "                                              steps_per_epoch = (len(trainset) // (2 * FLAGS.batch_size)) + 1)\n",
    "\n",
    "lrs = list()\n",
    "for i in range(20000):\n",
    "    lr_sched.step()\n",
    "    lrs.append(lr_sched.get_last_lr())\n",
    "    \n",
    "plt.plot(lrs)\n",
    "plt.xlabel('Batches')\n",
    "plt.ylabel('LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaadd3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b6f10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateStridedTimesteps(batch):\n",
    "    '''Inputs is B x T X C torch tensor of neural signals.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a38116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual train file\n",
    "\n",
    "from architecture import Model\n",
    "from torchaudio.models.decoder import ctc_decoder\n",
    "import logging\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# lsof /dev/nvidia* | awk '{print $2}' | xargs -I {} kill {}\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset=trainset, shuffle=True, num_workers=8, \n",
    "                       collate_fn = trainset.collate_fn,\n",
    "                       drop_last = False, batch_size=FLAGS.batch_size, pin_memory=True)\n",
    "\n",
    "\n",
    "device   = torch.device(\"cuda:{}\".format(0) if torch.cuda.is_available() else \"cpu\")\n",
    "n_chars  = len(trainset.text_transform.chars)\n",
    "model    = Model(trainset.num_features, model_size = FLAGS.model_size, num_layers = FLAGS.num_layers,\n",
    "                 n_conv_blocks = FLAGS.n_conv_blocks, num_outs = n_chars+1, dropout = FLAGS.dropout, \n",
    "                num_days = [len(session_mapping) if FLAGS.per_session_layer else None][0])\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params           = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(model)\n",
    "print(f'Number of parameters: {params * 1e-6} million')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "optim    = torch.optim.AdamW(model.parameters(), lr=FLAGS.learning_rate, weight_decay=FLAGS.l2)\n",
    "lr_sched = torch.optim.lr_scheduler.OneCycleLR(optim, max_lr = FLAGS.learning_rate, epochs = FLAGS.epochs,\n",
    "                                              steps_per_epoch = (len(trainset) // (2 * FLAGS.batch_size)) + 1)\n",
    "      \n",
    "#run = neptune.init_run(\n",
    "#    project=\"neuro/T12speech-transformers\",\n",
    "#    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjNmRjNDNhNS0yOGI0LTQ5MjAtODZiZi04Njc0NjA1ZDUwOWMifQ==\",\n",
    "#)\n",
    "\n",
    "#for key in FLAGS:\n",
    "#    run[f'hyperparams/{key}'] = getattr(FLAGS, key)   \n",
    "#run['trainable_parameters'] = params\n",
    "\n",
    "batch_idx = 0\n",
    "wers      = list()\n",
    "optim.zero_grad()\n",
    "for epoch_idx in range(FLAGS.epochs):\n",
    "    losses = []\n",
    "    for example in dataloader:\n",
    "        X_raw = addNoise(example['neural'], offset_strength = FLAGS.constantoffset_SD, \n",
    "                         whitenoise_strength = FLAGS.whitenoise_SD, randomwalk_strength = 0)\n",
    "        X_raw = X_raw.to(device, non_blocking=True)\n",
    "\n",
    "        #sess  = combine_fixed_length(example['session_ids'], seqlen).to(device)\n",
    "\n",
    "        pred = model(X_raw, example['session_ids'].to(device))\n",
    "        pred = F.log_softmax(pred, 2)\n",
    "      \n",
    "        # seq first, as required by ctc\n",
    "        y    = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device, non_blocking=True)\n",
    "        loss = F.ctc_loss(pred.swapaxes(1,0), y, example['lengths'] // 1, example['text_int_lengths'], blank=0)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if torch.isnan(loss):\n",
    "            assert False, \"NaN detected\"\n",
    "        \n",
    "        loss.backward()\n",
    "        if (batch_idx+1) % 2 == 0:\n",
    "            nn.utils.clip_grad_value_(model.parameters(), 10)\n",
    "            optim.step()\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            lr_sched.step() \n",
    "\n",
    "        batch_idx += 1\n",
    "        \n",
    "    train_loss       = np.mean(losses)\n",
    "    val_cer, val_wer = test(model, testset, device)\n",
    "    print(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f} validation PER: {val_cer*100:.2f}')\n",
    "    \n",
    "    run[\"train/loss\"].log(train_loss)\n",
    "    run[\"test/PER\"].log(100 * val)\n",
    "    #wers.append(val_wer * 100)\n",
    "    \n",
    "#run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbbb34f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21805d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f198861",
   "metadata": {},
   "source": [
    "* overall focus - optimize speed so that iteration cycles are faster\n",
    "\n",
    "TODO:\n",
    "   - rework testSubset() function\n",
    "   - try convolutional filter frontend to downsample 2-4x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc06552a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770f41f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5972a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b6ddecaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# speed optimization\n",
    "\n",
    "import neptune.new as neptune\n",
    "\n",
    "for name in list(flags.FLAGS):\n",
    "    delattr(flags.FLAGS,name)\n",
    "\n",
    "\n",
    "flags.DEFINE_boolean('debug', False, 'debug')\n",
    "flags.DEFINE_string('output_directory', '/oak/stanford/groups/shenoy/ghwilson/T12_transformer', 'where to save models and outputs')\n",
    "\n",
    "flags.DEFINE_integer('model_size', 512, 'Toggle S4 model in place of transformer') #768\n",
    "flags.DEFINE_integer('num_layers', 5, 'Toggle S4 model in place of transformer') # 6\n",
    "flags.DEFINE_integer('per_session_layer', 1, 'Toggle session-specific input layer')\n",
    "\n",
    "flags.DEFINE_float('dropout', 0.2, 'dropout strength') # 0.2\n",
    "flags.DEFINE_float('whitenoise_SD', 0.1, 'whitenoise strength')\n",
    "flags.DEFINE_float('constantoffset_SD', 0.1, 'whitenoise strength')\n",
    "flags.DEFINE_float('smoothing_sigma', 2, 'whitenoise strength')\n",
    "\n",
    "\n",
    "flags.DEFINE_integer('batch_size', 32, 'training batch size')\n",
    "flags.DEFINE_float('learning_rate', 3e-4, 'learning rate') #3e-4\n",
    "flags.DEFINE_integer('epochs', 150, 'training epochs')  #200\n",
    "flags.DEFINE_integer('learning_rate_patience', 5, 'learning rate decay patience')\n",
    "flags.DEFINE_string('start_training_from', None, 'start training from this model')\n",
    "flags.DEFINE_float('l2', 0, 'weight decay')\n",
    "\n",
    "flags.DEFINE_string('evaluate_saved', None, 'run evaluation on given model file')\n",
    "flags.DEFINE_string('lm_directory', '/oak/stanford/projects/babelfish/magneto/GaddyPaper/pretrained_models/librispeech_lm/', \n",
    "                    'Path to KenLM language model')\n",
    "flags.DEFINE_string('base_dir', '/oak/stanford/projects/babelfish/magneto/GaddyPaper/processed_data/',\n",
    "                    'path to processed EMG dataset')\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS([''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d008382d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "0.5508913993835449\n",
      "0.6132285594940186\n",
      "0.4028010368347168\n",
      "0.7902531623840332\n",
      "0.447542667388916\n",
      "0.47417426109313965\n",
      "0.5585513114929199\n",
      "0.4988820552825928\n",
      "0.6332972049713135\n",
      "0.7490522861480713\n",
      "0.4605426788330078\n",
      "0.37840819358825684\n",
      "0.7712693214416504\n",
      "0.5581281185150146\n",
      "0.44739460945129395\n",
      "0.8167107105255127\n",
      "0.8921349048614502\n",
      "0.5311856269836426\n",
      "0.6208529472351074\n",
      "0.4453761577606201\n",
      "0.823005199432373\n",
      "0.6504700183868408\n",
      "0.4757101535797119\n",
      "0.6977782249450684\n",
      "0.7509605884552002\n",
      "0.6015241146087646\n",
      "0.41062140464782715\n",
      "0.9088022708892822\n",
      "0.6767830848693848\n",
      "0.4757812023162842\n",
      "0.3979501724243164\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3255/42605145.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         X_raw = addNoise(example['neural'], offset_strength = FLAGS.constantoffset_SD, \n\u001b[0m\u001b[1;32m     34\u001b[0m                          whitenoise_strength = FLAGS.whitenoise_SD, randomwalk_strength = 0)\n\u001b[1;32m     35\u001b[0m         \u001b[0mX_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3255/3927735829.py\u001b[0m in \u001b[0;36maddNoise\u001b[0;34m(timeseries, offset_strength, whitenoise_strength, randomwalk_strength)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtimeseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddOffset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moffset_strength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwhitenoise_strength\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mtimeseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddWhiteNoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhitenoise_strength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrandomwalk_strength\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mtimeseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddRandomWalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomwalk_strength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3255/3927735829.py\u001b[0m in \u001b[0;36maddWhiteNoise\u001b[0;34m(timeseries, strength)\u001b[0m\n\u001b[1;32m     20\u001b[0m     strength (float)                     - noise standard deviation '''\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtimeseries\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# speed optimization\n",
    "\n",
    "import time\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "dataloader = DataLoader(dataset=trainset, shuffle=True, num_workers=8, \n",
    "                       collate_fn = trainset.collate_fn,\n",
    "                       drop_last = False, batch_size=FLAGS.batch_size, pin_memory=True)\n",
    "\n",
    "\n",
    "device   = torch.device(\"cuda:{}\".format(0) if torch.cuda.is_available() else \"cpu\")\n",
    "n_chars  = len(trainset.text_transform.chars)\n",
    "model    = Model(trainset.num_features, model_size = FLAGS.model_size, num_layers = FLAGS.num_layers,\n",
    "                num_outs = n_chars+1, dropout = FLAGS.dropout, \n",
    "                num_days = [len(session_mapping) if FLAGS.per_session_layer else None][0])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "optim    = torch.optim.AdamW(model.parameters(), lr=FLAGS.learning_rate, weight_decay=FLAGS.l2)\n",
    "lr_sched = torch.optim.lr_scheduler.OneCycleLR(optim, max_lr = FLAGS.learning_rate, epochs = FLAGS.epochs,\n",
    "                                              steps_per_epoch = (len(trainset) // (2 * FLAGS.batch_size)) + 1)\n",
    "\n",
    "\n",
    "batch_idx = 0\n",
    "wers      = list()\n",
    "optim.zero_grad()\n",
    "for epoch_idx in range(FLAGS.epochs):\n",
    "    losses = []\n",
    "    print('Epoch', epoch_idx+1)\n",
    "    for example in dataloader:\n",
    "        start = time.time()\n",
    "        X_raw = addNoise(example['neural'], offset_strength = FLAGS.constantoffset_SD, \n",
    "                         whitenoise_strength = FLAGS.whitenoise_SD, randomwalk_strength = 0)\n",
    "        X_raw = X_raw.to(device, non_blocking=True)\n",
    "\n",
    "        #sess  = combine_fixed_length(example['session_ids'], seqlen).to(device)\n",
    "\n",
    "        pred = F.log_softmax(model(X_raw, example['session_ids'].to(device)), 2)\n",
    "      \n",
    "        # seq first, as required by ctc\n",
    "        y    = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device, non_blocking=True)\n",
    "        loss = F.ctc_loss(pred.swapaxes(1,0), y, example['lengths'] // 1, example['text_int_lengths'], blank=n_chars)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if torch.isnan(loss):\n",
    "            assert False, \"NaN detected\"\n",
    "        \n",
    "        loss.backward()\n",
    "        if (batch_idx+1) % 2 == 0:\n",
    "            nn.utils.clip_grad_value_(model.parameters(), 10)\n",
    "            optim.step()\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            lr_sched.step() \n",
    "            print(time.time() - start)\n",
    "\n",
    "        batch_idx += 1\n",
    "    \n",
    "        if batch_idx == 100:\n",
    "            assert False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3348079",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3255/1654622639.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "del model, X_raw, y, pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c976269a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 1 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/neuro/T12speech-transformers/e/TSPEEC-19\n"
     ]
    }
   ],
   "source": [
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387a8065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "603809e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 1 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/neuro/T12speech-transformers/e/TSPEEC-10\n"
     ]
    }
   ],
   "source": [
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6151bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "b4134c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testSubset(model, testset, sessions, device):\n",
    "    '''Evaluate on subset of days'''\n",
    "    model.eval()\n",
    "\n",
    "    blank_id = len(testset.text_transform.chars)\n",
    "    \n",
    "    #if testset.togglePhones:\n",
    "    if False:\n",
    "        lexicon_file = 'cmudict.txt'\n",
    "    else:\n",
    "        lexicon_file = os.path.join(FLAGS.lm_directory, 'lexicon_graphemes_noApostrophe.txt')\n",
    "\n",
    "    decoder = ctc_decoder(\n",
    "       lexicon = lexicon_file,\n",
    "       tokens  = testset.text_transform.chars + ['_'],\n",
    "       lm      = os.path.join(FLAGS.lm_directory, '4gram_lm.bin'), \n",
    "       blank_token = '_',\n",
    "       sil_token   = '|',\n",
    "       nbest       = 1,\n",
    "       lm_weight   = 2.1, # default is 2; Gaddy sets to 1.85\n",
    "       #word_score  = -3,\n",
    "       #sil_score   = -2,\n",
    "       beam_size   = 50  # SET TO 150 during inference\n",
    "    )\n",
    "\n",
    "    dataloader  = torch.utils.data.DataLoader(testset, batch_size=1, collate_fn=testset.collate_fn)\n",
    "    references  = []\n",
    "    predictions = []\n",
    "    sessions_cers = dict(zip(sessions, [[] for session in sessions]))\n",
    "    with torch.no_grad():\n",
    "        for i, example in enumerate(dataloader):\n",
    "            if example['session_ids'][0] in sessions:\n",
    "                idx     = example['session_ids'][0].item()\n",
    "                X_raw   = example['neural'][0].unsqueeze(0).to(device)\n",
    "                session = example['session_ids'][0].unsqueeze(0).to(device)\n",
    "                pred    = F.log_softmax(model(X_raw, session), -1).cpu()\n",
    "\n",
    "                beam_results = decoder(pred)\n",
    "                pred_int     = beam_results[0][0].tokens\n",
    "                pred_text    = ' '.join(beam_results[0][0].words).strip().lower()\n",
    "                target_text  = testset.text_transform.int_to_text(example['text_int'][0])\n",
    "\n",
    "                if len(target_text) > 0:\n",
    "                    sessions_wers[idx].append(100 * jiwer.wer(target_text, pred_text))\n",
    "                    \n",
    "\n",
    "    model.train()\n",
    "    return sessions_wers\n",
    "\n",
    "\n",
    "def getBootstrappedCIs(data, alpha, n_bootstraps = 10000):\n",
    "    \n",
    "    means = np.random.choice(data, (n_bootstraps, len(data)), replace = True).mean(axis = 1)\n",
    "    upper = np.percentile(means, 100 - (alpha/2))\n",
    "    lower = np.percentile(means, alpha/2)\n",
    "    \n",
    "    return lower, upper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "f4c89f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean WER = 0.2773759018759019\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get WERs for subset of sessions tested in Nature paper:\n",
    "\n",
    "sessions_wers = testSubset(model, testset, sessions = [15, 16, 18, 19, 20], device = device)\n",
    "\n",
    "means = list()\n",
    "for key, value in sessions_wers.items():\n",
    "    lower, upper = getBootstrappedCIs(value, alpha = 5, n_bootstraps = 10000)\n",
    "    plt.plot([key, key], [lower, upper], c = 'coral')\n",
    "    means.append(np.mean(value))\n",
    "plt.plot(sessions_wers.keys(), means, c = 'coral')\n",
    "plt.xlabel('Session ID')\n",
    "plt.ylabel('WER')\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.axhline(11.8, linestyle = '--', c ='k', alpha = 0.5)\n",
    "\n",
    "print('Overall mean WER =', np.mean(np.concatenate([x for x in sessions_wers.values()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368df5b2",
   "metadata": {},
   "source": [
    "TODO implementation:\n",
    "- get hydra working\n",
    "- add softsign activation to day-specific input layer (paper suggestion)\n",
    "- add both pre and post-activation dropout to day-specific input (paper suggestion)\n",
    "- try L2 regularization (paper suggestion)\n",
    "- stack windowed bins together as input features (paper suggestion) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "45b1b8d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1237b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7f6775ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "dataloader = DataLoader(dataset=trainset, shuffle=True, num_workers=0, \n",
    "                       collate_fn = trainset.collate_fn,\n",
    "                       drop_last = False, batch_size=FLAGS.batch_size, pin_memory=True)\n",
    "\n",
    "def old():\n",
    "    for i, example in enumerate(dataloader):\n",
    "        schedule_lr(batch_idx)\n",
    "\n",
    "        #X_raw = addNoise(example['neural'], offset_strength = 0, whitenoise_strength = 0.2, randomwalk_strength = 0)\n",
    "        X_raw = example['neural'].to(device, non_blocking=True)\n",
    "        y     = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device, non_blocking=True)\n",
    "\n",
    "        if i == 50:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2d0e83a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 20.23 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "18.3 s ± 16.5 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit old()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7c7a21d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2 s ± 110 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit old()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89a0a91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Magneto (3.9)",
   "language": "python",
   "name": "mag-3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
