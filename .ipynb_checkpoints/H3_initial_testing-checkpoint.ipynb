{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e1c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchaudio.models.decoder import ctc_decoder\n",
    "\n",
    "from architecture import Model, S4Model, H3Model\n",
    "from data_utils import combine_fixed_length, decollate_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de1c4dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LanguageTask:\n",
    "    \"\"\"Base class for language tasks.\n",
    "\n",
    "    Each task should implement the following methods:\n",
    "        __init__: Initialize the task.\n",
    "        generate_sequence: Generate a sequence of tokens for the task.\n",
    "    \"\"\"\n",
    "\n",
    "    def generate_sequence(self):\n",
    "        \"\"\"This is a method must be implemented by each task.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_embedding_layer(self, embedding_dim):\n",
    "        \"\"\"Return an embedding layer for the task vocabulary.\"\"\"\n",
    "        return nn.Embedding(len(self.alphabet), embedding_dim)\n",
    "\n",
    "    def encode_sequence(self, sequence):\n",
    "        \"\"\"Convert a sequence of tokens to a tensor of indices.\"\"\"\n",
    "        return torch.tensor([self.vocab[token] for token in sequence])\n",
    "\n",
    "    def decode_sequence(self, sequence):\n",
    "        sequence = sequence.tolist()\n",
    "        \"\"\"Convert a tensor of indices back into a sequence of corresponding tokens.\"\"\"\n",
    "        return [self.inv_vocab[token] for token in sequence]\n",
    "\n",
    "    \n",
    "    \n",
    "class InductionHeadTask(LanguageTask):\n",
    "    \"\"\"Toy language task to test for \"copying\" capabilities.\n",
    "\n",
    "    The task is to learn to repeat the token that is shown after the special token '_'.\n",
    "    We use an alphabet of 19 standard letters, plus the special token.\n",
    "    The model is trained on sequences of 30 tokens. In each sequence, the pair ('_' + letter) is shown twice,\n",
    "    and all other tokens are sampled randomly with replacement from the set of standard letters.\n",
    "\n",
    "    Example sequences:\n",
    "        'a _ b c a c _ b'\n",
    "        'b _ a c b c _ a'\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"induction_head\"\n",
    "    seq_length = 30\n",
    "    normal     = list(\"abcdefghijklmnopqrs\")\n",
    "    special    = \"_\"\n",
    "    alphabet   = normal + [special]\n",
    "    vocab      = {token: idx for idx, token in enumerate(alphabet)}\n",
    "    inv_vocab  = {idx: token for token, idx in vocab.items()}\n",
    "\n",
    "    def generate_sequence(self):\n",
    "        answer = self.special + np.random.choice(self.normal)\n",
    "        base = list(np.random.choice(self.normal, self.seq_length - 4, replace=True))\n",
    "        base = list(np.random.permutation(base + [answer])) \n",
    "        return list(\"\".join(base + [answer])) # convert to list of chars\n",
    "\n",
    "    \n",
    "def makeBatch(task, batch_size = 16):\n",
    "    seqs = torch.stack([task.encode_sequence(task.generate_sequence()) for i in range(batch_size)], dim = 0)\n",
    "    return seqs.unsqueeze(-1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c13e4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H3Model(\n",
      "  (encoder): Linear(in_features=1, out_features=256, bias=True)\n",
      "  (h3_layers): ModuleList(\n",
      "    (0): H3(\n",
      "      (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (s4d): S4(\n",
      "        (kernel): SSKernel(\n",
      "          (kernel): SSKernelDiag()\n",
      "        )\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dropout): Identity()\n",
      "        (output_linear): Sequential(\n",
      "          (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLU(dim=-2)\n",
      "        )\n",
      "      )\n",
      "      (shift): Shift()\n",
      "    )\n",
      "    (1): H3(\n",
      "      (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (s4d): S4(\n",
      "        (kernel): SSKernel(\n",
      "          (kernel): SSKernelDiag()\n",
      "        )\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dropout): Identity()\n",
      "        (output_linear): Sequential(\n",
      "          (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (1): GLU(dim=-2)\n",
      "        )\n",
      "      )\n",
      "      (shift): Shift()\n",
      "    )\n",
      "  )\n",
      "  (norms): ModuleList(\n",
      "    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (dropouts): ModuleList(\n",
      "    (0): Dropout1d(p=0.0, inplace=False)\n",
      "    (1): Dropout1d(p=0.0, inplace=False)\n",
      "  )\n",
      "  (linears): ModuleList()\n",
      "  (w_out): Linear(in_features=256, out_features=20, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from absl import flags\n",
    "\n",
    "task = InductionHeadTask()\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS([''])\n",
    "\n",
    "FLAGS.num_layers = 2\n",
    "FLAGS.model_size = 256\n",
    "FLAGS.dropout    = 0.\n",
    "\n",
    "model = H3Model(num_features = 1, num_outs = len(task.alphabet)).to('cuda')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f07d4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch, Time, Channels: torch.Size([16, 30, 1])\n",
      "torch.Size([16, 256, 30])\n",
      "torch.Size([16, 256, 30])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55392/4157440144.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Batch, Time, Channels:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/projects/silent_speech/architecture.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_feat, x_raw, session_ids)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;31m# Apply H3 block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0mz\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;31m# Dropout on the output of the S4 block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/silent_speech/h3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m#k = rearrange(self.k_proj(x), \"b h l  ->  h l b\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m#v = rearrange(self.v_proj(x), \"b h l  ->  h l b\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mshift_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;31m#s4d_out   = self.s4d(v * shift_out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0ms4d_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms4d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mshift_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/silent_speech/h3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, u)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mB_fc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mC_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mirfft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB_fc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mC_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;31m#kernel = rearrange(kernel[..., :L], \"b h l -> b l h\") # orig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "batch = makeBatch(task).to('cuda')\n",
    "print('Batch, Time, Channels:', batch.shape)\n",
    "\n",
    "model.forward(None, batch, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8976bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Magneto (3.9)",
   "language": "python",
   "name": "mag-3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
